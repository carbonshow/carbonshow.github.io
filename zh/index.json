[{"content":"  数据处理在 Session-Based 场景中的一大特点是：同一会话产生的数据应投递到同一处理单元中。 不同于无状态服务，其难点在于：当集群发生扩缩容时，除了要保证负载均衡，更要保证数据到处理单元的稳定映射。 本文从经典案例入手，介绍 Sharding 技术是如何解决该问题的，并结合 kafka consumer 浅谈应用方法。\n Session-Based 路由问题分析  什么是 Session   数据流处理面临四个基本问题1 :\n  What   数据处理结果是什么？比如求和，统计，机器学习等由业务场景决定的计算逻辑。\n  Where   数据何时产生（Event Time）？数据的生成、观测、传输、计算一般分布在不同的链路，存在时间差。Event Time 用于还原事件发生的原始时刻与顺序。\n  When   数据何时处理（Processing Time）？原始数据不断涌入到处理节点，需按照一定策略加工才能得到结果，比如：来一个处理一个；来一批集中处理等，涉及到延迟和性能的平衡。Processing Time 用于描述原始数据转变为结果的方法。\n  How   数据采用何种更新方式？因为数据像水流般逐渐到来，分批处理后需将阶段性结果再加工才能得到最终结果。汇总的方式可能有：总是用最新结果覆盖旧数据（Discarding），累积（Accumulating），累积并记录变化量（Retracting）等。\n  Session 是 Where 问题的一种表现形式：事件并非相互独立，而是存在层级关系，某些事件隶属同一上下文，需作为整体处理才能得到正确结果。比如，用户在线时的所有操作，属于同一 session；它以用户登录为起点，下线为终点。同一用户同一时刻只能保留唯一 session ，因为一些服务可能依赖于全局状态。下面结合游戏场景举个具体例子。\n  应用举例——消极比赛  MOBA 或其它竞技类型游戏，核心玩法以单局形式存在，每个单局可视为一个 session。单局除了基本游戏体验，还有一些旁路功能，比如判定作弊、消极比赛等。\n Judge Cluster 是消极比赛裁决服务集群，包含多个进程实例（Instance）。每个 Instance 上运行多个裁决器，裁决器和具体单局 Game i 一一对应。 Game i, Event j 表示单局 i 中的第 j 条事件。消极判定依赖事件上下文，所以单局事件须 完整、有序 交给 同一对应裁决器 处理。因为判定逻辑依赖行为的因果关系，因果关系既和 Event Time 顺序有关，也和事件完整性有关。比如角色在战斗过程中死亡，是正常行为；但如果站着不动让别人攻击，那就有消极嫌疑。这种场景对架构设计提出以下要求：\n  必须满足\n  单局事件可被正确路由到裁决器中。\n  路由映射需保证稳定。\n  数据有序。\n    可选\n  数据丢失可恢复。不可恢复时，受影响的单局不判定消极行为，对正常用户无影响，体验可接受。\n    这几条要求代表了 Session 场景数据投递的技术点，下文对其抽象，尝试提炼通用解决方案。\n  问题抽象    Message\n  Message Key 消息的唯一标记\n  Session Key 消息隶属的 session 唯一标记\n  Sequence ID 消息序列号，用于排序，可以是时间戳。\n    Cluster\n  Node 进程实例\n  Cluster 一组 Nodes 的集合，作为整体对外提供服务。\n  Entity 消息处理实体，和 session 对应，集群内唯一。\n    Session\n  存在持续期：起始（Start）和终止（Stop）。start 对应 Entity 创建，stop 对应 Entity 销毁，procesing 即 Entity 处理消息。\n  同一 session 内的消息必须由同一 Entity 处理。\n  消息到 Entity 的映射关系是稳定的。\n  Entity 和 Node 的映射关系可能动态调整，比如 Cluster 扩缩容，Node 消息不可达或响应过慢导致的 Entity 在 Nodes 上分配的重新调整（Rebalance）。\n  Entity 是否需要根据 Event Time 的顺序处理消息和场景相关，如果关注则应关注 Sequence ID。\n      问题分析   核心问题只有一个： 确保 Session 中的消息被正确投递到对应的 Entity 中 。看似简单，但 Entity 的载体是 Nodes ，Nodes 受各种条件影响变化几乎是不可避免的。难点就转换为：怎么保证 Entity 的逻辑地址和物理地址（所属 Node 地址）解耦，不变的逻辑地址如何自动化更新对应的物理地址。\n 我们先来看下，哪些情况会引起 Nodes 和 Entity 映射关系的变化。\n  Entity 的创建和销毁，对应于 Session 生命周期的变化。\n  Cluster 变化。Nodes 增加或者减少，导致 Entity 迁移到新的 Node，原因可能有：\n  扩缩容。如果基于容器，使用编排系统自动调度，可能会更频繁一些。\n  容灾，Node 故障或者响应过慢。\n  更新。\n    Rebalance 时更新 Nodes 和 Entity 的映射关系，需要考虑的因素和应用场景相关，常见的有：\n  路由更新 。通过算法，存储等方式，确定消息要发给哪个 Node。\n  负载均衡 。提供持续服务基础要求。\n  数据本地性（Locality) ，分空间和时间两种。比如 Kafka Producer 根据 Session Key 向 Partition 推送数据，同一 session 中的数据会进入同一 Partition，作为 consumer 应尽可能保证拉取的 session 数据在本地处理，否则要多一次转发。\n   上图展示了 session 场景下数据路由的基本流程，总结起来 核心环节 有两个：\n  根据消息确定 Entity ID。\n  监听 Cluster 变化，根据 Entity ID 获取所属节点位置。\n  下面我们来看看解决方案。\n  已有方案    (a) 直连   Service Client 作为请求端，配置后端集群各个 Node 地址，本地实现路由功能。优势是：和业务场景关联密切，灵活性高。劣势是：无法自动感知后端集群变化；对请求方不友好，需要侵入式集成 SDK；\n  (b) 借助第三方   为了自动感知后端集群变化，可以让第三方 Service Registry 监管，请求端从 Service Registry 查询即可 。比如：常用的服务发现组件 zookeeper, etcd 等；目录服务，域名解析服务；数据库等等。\n  (c) 网关中转   请求方不必集成复杂 SDK，将路由和服务发现集成到网关中。优势是：侵入性小；服务间解耦，不必关注集群变更细节，只需关注服务名。缺陷是：通用网关承载通用逻辑，个性化路由需要个性化网关增加复杂度；通信链路多了一次 hop 。\n  (d) Cluster 内路由   将复杂的路由隐藏到 Cluster 内部，内部节点承担数据转发功能。相比网关中转优势在于：外部只需要考虑负载均衡，不必担心路由；最差情况才会多一次 hop 。\n  上述四种方法，各有适用场景，相互间也并非完全独立可以组合使用。为了简化问题，下文将针对 (d) Cluster 内路由 说明 Sharding 的一种实现方案。在这种场景下，外界对 Cluster 访问简化为：\n  服务发现。\n  负载均衡。Node 级别粗粒度，算法很多，比如：随机、Round Robin、最小负载等。\n  而 Session 消息到 Entity 的稳定路由则全部由 Cluster 内部的 Sharding 机制解决。\n    Cluster Sharding   Cluster Sharding 在集群内部实现消息路由，无论哪个节点接收到外部投递的消息，都应准确发送到目标 Entity 所属 Node。这需要提根据消息自身内容获取 Node 位置，并随着 Cluster 伸缩自适应更新。这种方式对外屏蔽了路由的复杂性，而且 Cluster 本来就要处理消息，可以做各种自定义逻辑，提高了灵活性。下面本文参考Akka Cluster 介绍 Sharding 的体系结构和术语，然后描述 routing, rebalance 过程。 体系简介     符号 全称 说明     ST Sharding Type 一个独立的 Sharding 体系，由两个因素决定：处理的消息类型及名称。   E Entity 消息处理实体，一个 session 的所有消息均投递给该 Entity 处理。   S Shard 包含一组相同功能的 Entity，一个 Shard 只能位于同一 Node 上，负责内部 Entity 创建销毁及路由。   SC Shard Coordinator 每个 ST 一个，负责当前体系内 Shard 与 Node 映射关系的维护：Allocate, Rebalance。   SR Shard Region 每个 Node 一个，负责 Shard 创建和路由，本地 Shard 直发，远程 Shard 转发     Sharding 术语介绍    一个 Sharding 体系包含：/Entity, Shard, Shard Regin, Shard Coordinator/ 四个模块。Sharding 体系的区分取决于逻辑功能。比如：单局战斗事件和聊天事件，处理逻辑显然不同，通过消息类型即可区分；但同样是战斗事件，可用于消极行为裁决，也可用于数据统计，这就需要用名称区分。所以独立的 Sharding 体系可以将 消息类型 和 名称 组合作为唯一标记。Sharding 体系确定后，就定义了一类具有相同功能的 Entity 集合，这里用 EntityTypeKey[MsgType](name) 来表示。MsgType 表示这类 Entity 可以处理的消息类型，name 是字符串标记。\n Entity 数量可能很多，百万用户在线时一个场景通常会有数十万。直接对 Entity 管理代价很高，所以模拟现实世界中组织架构的方式 *分层*。将 Entity 分成若干组，以组为基本单位管理，这就是 Shard 。对 Entity 的访问退化为 Shard 访问，粒度从细变粗。考虑容灾、扩展等多方面因素，Shards 都不应存放于同一 Node，一定分散在不同 Nodes 上。这时就面临两个问题：\n  从 Shard 角度看：我应该在哪个 Node 上？这是 Allocate, Rebalance 无可回避的问题。\n  从 Node 角度看：我到底管理了哪些 Shard? 不在我管辖范围内 Shard 在哪儿？\n  这两个问题就分别需要 Shard Coordinator 和 Shard Region 解决。\n  Shard Coordinator   当 Shard 创建及再平衡时，决定 Shard 和 Node 的映射关系。这种决策需要 Cluster 全局信息，且要做统一判断，所以 SC 是 Cluster 内全局唯一的处理逻辑。\n  Shard Region   每个 Node 一个。作为当前 Node 所有 Shards 的管理器，扮演三种功能：\n  Cluster 内，和 SC 通信确定 Shard 位置，和其它 Node 上的 SR 通信路由消息。\n  对 Cluser 外，接收消息请求，转发到正确的 Shard 上。\n  Node 内，创建销毁 Shard，并转发消息。\n    Shard 数量通常是固定的，每个 Shard 中 Entity 的数量是动态变化的。这点 **非常关键**，这为 Entity 到 Shard 的稳定映射提供了可能。因为 Node 是物理存在，它的变化是不可避免的，Shard 是逻辑存在，可以稳定不变。这样可以保证 Session 和 Node 解耦，简化了 Session 到 Shard 映射复杂度。为了保证 Shard 能够较好均匀的分布在所有 Nodes 上，可以将 Shard 数量设置为 Nodes 总数的较大上限，比如 Nodes 数量的十倍。\n 该体系的具体运作方式，我们结合两个关键流程介绍：路由和再平衡。   路由   路由有两个过程：不存在时创建；存在时转发。根据目的地有两种场景：目标 shard 位于收到消息的 Node 本地；目标 shard 所在 Node 并非收到消息的 Node 。下面分别介绍这两种场景。 本地路由    Node A 收到消息 Msg1，转交给 SR 处理。Msg 的类型是 Type(GameID, SeqID, GameData) ：GameID 表示单局 ID，也就是 Session Key(Session ID)；SeqID 表示单局消息的顺序；GameData 表示消息的数据内容。\n  SR 从 Msg1 获取 Entity ID(E1)和 Shard ID(S1):\n  获取 Entity ID，一般情况下 Entity 和 Session 一一对应，所以 Entity ID 就是 Session ID。Session ID 包含在 Message 中，比如在消极比赛裁决中，就是 GameID。\n  根据 EntityID 获得 ShardID。这点非常关键，需要保证 Entity 到 Shard 稳定映射，由于 Shard 总数是不变的，一个简单有效的方式是对 Shard 总数取模，这个结果也总是稳定的。\n    SR 如果是第一次遇到 S1，是不知道位置的，需要向 SC 发起查询。如果已经查询过，且这段时间内没有发生变化，那么直接使用本地缓存结果即可。\n  SC 向 SR 返回查询结果：S1 在 Node A 上。\n  SR 发现 S1 在本地，那么创建 S1。\n  S1 继续创建 Entity: E1，并将消息路由给 E1。\n  上述过程描述了，Node 收到消息并在本地创建 Shard 和 Entity 的过程；如果已经创建好，那么本地路由就不必再和 SC 交互，直接转发给本地 Shard 即可，因为 SR 保留了本地路由信息。\n  远程路由   远程路由和本地路由的大概流程类似，区别点在于：当 SR_A 向 SC 查询后，发现目标 Shard(S2)在节点 B 上，那么将消息转发给 Node B。SR_B 在本地完成 S2 的创建和路由。在这种情况下，消息投递多了一次 Hop。如果要考虑优化，有两个方面：\n  SC 在做分配时除了负载均衡，还应考虑 Locality，减少 Node 间消息转发。如果某个 Node 收到消息那么尽可能将 Shard 分配到该 Node 上。\n  消息投递到 Cluster 时，虽然无论发送给哪个 Node 都可以保证消息投递，但如果稳定发送给固定 Node，则可以保证 Locality 更好的发挥作用。\n      再平衡   Cluster 内部 Nodes 状态变化，比如增加、移除、不可达，必然涉及 Shard 和 Node 映射关系的调整，这就是再平衡。再平衡具体包含两种情况：\n  增加 Node。Shard 需要从旧 Node 迁移到新 Node，或者新 Shard 直接在新 Node 创建。\n  移除 Node。这个过程多数是不可控的，相当于在剩余 Nodes 上重新创建 Shard，可以参考前文介绍的创建过程。\n  因此再平衡主要考虑 两个问题 ：\n  增加 Node 时，Shard 和 Node 的映射关系如何更新。\n  Shard 状态如何迁移。\n新增 Node       SC 感知到新增 Node，根据 Rebalance 策略，计算出需要将 Node B 上的 Shard(S1) 迁移到新 Node 上，并对新 Node 初始化——创建 Shard Region，S1\u0026#39;。然后开启整个迁移流程。\n  暂停所有 SR 关于 S1 位置的查询\n  通知所有 SR 停止向 S1 发送消息，将输入全部缓存在本地。目的在于让停止向 S1 发送新消息，让它处理完残留后再向新位置迁移。\n  S1 处理完所有消息后，告知 SC 自己已经没有遗留工作，可以关闭。对于有状态的 shard，还需要完成状态到 S1\u0026#39; 的同步。之后 SC 认为迁移流程结束。\n  SC 关闭 Node B 中的 S1.\n  SC 恢复 SR 关于 S1 位置的查询，并主动通知 S1 新地址——S1\u0026#39;。\n  缓存的消息或者新进消息，根据 S1\u0026#39; 新地址被路由到新 Node。\nShard 状态迁移     如果 Shard 中 Entity 是有状态的，且状态不可丢，那么需要将 Entity 的状态同步到新节点。不过同步方式和业务场景密切相关，不应该也没必要由 Shard 底层提供解决方案，只要做好流程控制即可。一般有两种方式：\n  \u0026lt;Inputs, replay\u0026gt;   将导致状态变更的消息（Inputs）按序保存，并持久化到第三方存储，比如消息中间件 kafka。新 Entity 同步时将这些事件拉取到本地重播（replay）。这种方式要求内部逻辑能够依据输入完全恢复，在处理随机种子，绝对时间戳等情况时需要非常谨慎，如果消息量较大恢复时间较长。一个典型例子就是 Binlog。\n  \u0026lt;Status, set\u0026gt;   将状态（Status）保存起来，类似一张快照（Snapshot），新 Entity 将快照数据设置到本地即可。这种方式保存的数据量可能较大，而且对更新频率要求较高否则会导致信息丢失。\n  实际应用时建议结合需求处理：\n  session 数据真的不可丢吗？有损服务是否可以接收？至少消极行为裁决，是不需要在迁移时状态同步的。最差情况就是迁移过程中一些消极行为没有检测到，但这比例很低。\n  session 数据不可丢。需结合业务特点判断 Binlog 和 Snapshot，哪种方案更适合，甚至两者结合。\n    顺序   细心的读者，可能会发现一个问题。如果任何一个 Session 的数据随意发送给任何一个 Node，虽然最终路由给正确的 Entity，但顺序是无法保证的。这实际上是网络通信的基本问题。从 A 到 B，如果有多条通路，那么无法保证 A 发送消息的顺序和 B 接收消息的顺序一致。这时有两种解决方案：\n  保证通信链路是单一的。对 Cluster Sharding 而言意味着，同一 Session 的消息应该发送给同一 Node，任意一个 Node 皆可，但应保持不变。\n  Entity 收到消息后重新排序。需要维持一定的缓存空间，可能会增加处理延迟。\n  下面结合消极行为裁决，我们来看看如何利用 sharding 解决各种问题。\n    Kafka 应用   我们再回顾下游戏单局数据处理的例子：应用举例——消极比赛。单局数据有多个应用场景，比如：单局结算、玩家生涯指标、大盘数据统计、消极行为裁决。这是典型的 Publish-Subscribe 场景，可以使用 Kafka 保存单局事件，做到不同服务间的解耦。由于消极行为裁决要求保留消息的原始顺序，所以在通信链路上应该保证唯一性。\n Game Cluster 是单局服务集群，一个单局只会存在于一个 Node 上，比如 Game 3 在 Node 2 上。Game 3 作为 Session，将 Game ID 作为 Sessioin ID，并作为消息 Key 推送到 Kafka 中。因此可以保证，同一 Game 的所有消息会 按序 进入同一 Partition 。裁决服务集群作为 Consumer Group，从 Kafka 拉数据。每个 Partition 只会由一个固定的 Consumer 消费，在 Consumer Group 稳定的情况下，Partition 到 Consumer 的映射关系是稳定的，到 Entity 的链路也是稳定的。因此可以做到 Entity 3 稳定有序 的消费 Game 3 产生的数据。\nConsumer Rebalance   这样似乎就够了，但实际生产环境很难保证 Consumer Group 维持不变，比如：\n  Consumer Group 扩容\n  Consumer 消费速度过慢，被 kafka 踢掉\n  Consumer 故障\n  如果 Consumer 基于 K8s 托管，可能根据资源使用情况调整容器位置\n   如图所示，Partition 3 本来由 Consumer 1 消费。Game 1 中的事件按先后顺序分别是 M1,M2,M3。当 M1 和 M2 被 C1 消费后，Consumer 3 加入成功，Partition 3 被分配给 C3。那么 Game 1 中的 M3 则转由 C3 处理。显然 G1 数据被截断分拆给不同的 Consumer 处理，结果是不正确的。\n  Sharding   我们期望的效果是 Partition 无论被哪个 Consumer 消费，同一 Session 的数据总能转发给同一 Entity 处理。经过前面的介绍，sharding 是非常适合的，应用流程如下所示：\n  单局的消息类型定义为 GameMsgType(GameID, SeqID, GameData) ，GameID 是单局的唯一标记，等价于 Session ID.\n  单局服务将 GameID 作为消息 Key 推送到 Kafka 中。保证了同一单局内的消息按序进入同一 Partition.\n  消极行为裁决服务作为消费端接入 Kafka。需要启用 Sharding 机制，先定义 Sharding 体系 EntityTypeKey ，消息类型是 GameMsgType ，名称是 Consumer Group ID。这样可以保证在 Kafka 的所有消费者中唯一。\n  定义 Extractor，从消息中获得 EntityID。需要能够体现 Session 和 Entity 的稳定映射关系，在当前场景下，可以视为 GameID。\n  根据 EntityID 获取 ShardID: ShardID=EntityID%ShardCount(\u0026gt;= PartitionCount)，ShardCount 是一个逻辑概念，不应小于 Partition 数量。\n  再平衡策略。当 SC 接收 SR 请求为 Shard 分配 Node 时，优先保证分配给接收消息的 Node。好处在于 consumer 收到消息后收到消息后直接本地处理，减少一次网络转发。\n      总结   Sharding 技术，旨在实现逻辑处理单元和物理节点的解耦，将复杂的路由逻辑隐藏在集群内部实现，降低外部访问的复杂度。如果配合消息队列以及其它数据恢复技术，还可支持消息有序，节点状态迁移，比较适合做为分布式系统中细粒度有状态服务的路由解决方案。\n  Footnotes   1 Tyler Akidau, Slava Chernyak \u0026amp; Reuven lax. Streaming Systems: The What, Where, When and How of Large-Scale Data Processing[M].O\u0026#39;REILLY, 2018-07-12.\n    ","description":"介绍Cluster Sharding 的基本原理以及在kafka consumer 场景中的应用","id":0,"section":"posts","tags":["sharding","kafka","cluster"],"title":"Cluster Sharding及kafka应用","uri":"https://carbonshow.github.io/zh/posts/cluster-sharding-kafka/"},{"content":"  小虫飞飞大本营 折腾了一段时间终于搞定了，虽然简陋但总算有个落脚的地方。开荒时遇到了不少问题，记录下来，算是一种纪念吧。\n 初衷   搭建个人技术博客，目的有两个：\n  知识整理。在工作或者闲暇时，大家总会遇到各种问题和感兴趣的技术点。俗话说好记性不如烂笔头，及时归纳总结有助回顾。这样一种情况想必大家遇到过：某个问题心里很清楚，但讲出来时发现不够周密，有遗漏。这正应了费曼学习法 ：用写作、讲授来倒逼学习。\n  以“技”会友。独乐不如众乐，有交流探讨才能更快成长。更多人看到，就会有更多人评价，不仅有助于发现自己的纰漏，也能帮助更多的人少走弯路。\n  为了实现上面的目标， 小虫 做了一遍调研，发现有以下几种途径：\n  微信公众号。传播便捷，内容质量要求高，还能有收益。\n  现成的博客平台。耳熟能详的有：CSDN，简书，博客园；近几年兴起的：掘金，SegmentFault；问答形式的知识平台：知乎等。它们的好处是：\n  体系成熟。无论是编辑，查询，搜索，互动，收益，一应俱全。\n  搜索引擎友好。基本都做到了极致，这意味着可以跳脱出固有圈子，让更多人阅读。\n    自己搭建。最大的好处是树立个人品牌，代价是——全部从零开始：服务器，域名，备案，数据库，后台进程等一整套。当然现在也有不少云服务商提供配套解决方案，比如：阿里，腾讯，netlify 等。这应该是最难走的一条路，但也是不少技术大牛采取的方式。\n  基于已有静态页面托管平台。典型例子就是 github ，域名和服务器都有，只缺内容。\n  经过评估，小虫最终决定从静态页面开始，把心思聚焦在内容上，等真正有所积累再慢慢向外推广。那么问题剩下两个：\n  选择一个静态页面生成工具\n  选择一个页面托管平台\n    静态页面生成器   简称 SSG(Static Site Generator)，简而言之一句话：让作者把精力聚焦在内容上，将 Markup 语言写出的普通文本转化为丰富多彩的 web 页面。这个需求可以说直击广大技术宅的痛点，您瞧现有的成熟解决方案就多达 400+ 。2019最佳的10个SSG ，再加上国内大名鼎鼎的 hexo 基本补全了优秀候选列表。 小虫 最终选择了 hugo ，原因很简单：\n  基于 go 开发，一个二进制就行，没有错综复杂的依赖关系。\n  社区活跃。这点很重要，用的人越多，解决方案也就更多。\n  功能完善。前端语言支持的种类丰富，比如 小虫 习惯使用的 Org-Mode ，已经内置支持；整体框架灵活易懂，比如文档分类，多语言，主题，内置 shortcode 支持更强大的表现力等；修改即时预览；一键部署等。\n  说了这么多还是直接上图吧，我们期望的就是： 些普通文本，看 web 页面  源文本演示:    本文基于 Emacs 使用 Org-Mode 编写，所有内容均来自于该文件，生成的 web 页面就是您当前看到的：\n Web 页面展示:    官网有快速开始的详细说明， 小虫 以自己的实际情况为例，介绍重点。\n安装及创建   Mac  安装\n1  brew install hugo   创建新网站，根目录：小虫飞飞\n1  hugo new site 小虫飞飞       'use strict'; var containerId = JSON.parse(\"\\\"d5cc742a70f05b46\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; } \n  配置   hugo 框架完善且庞大，如果根据文档逐条学习，虽然可以获得整体认识，但在实际应用中依然会碰到很多问题。因为不同主题在 hugo 基础上多多少少做了些个性化的封装，且使用方式差异较大。所以 小虫 建议： 先选主题，在主题框架内学习配置 。与实践结合，提高了学习效率。 主题挑选可以在 Hugo主题官网 查阅， 小虫 使用的是 zzo 。下面以 zzo 为例简述使用过程：\n zzo  安装主题 1  git submodule add ttps://github.com/zzossig/hugo-theme-zzo.git themes/zzo   参考示例配置自己的工程 1 2 3  cp -rf themes/zzo/exampleSite/config . cp -rf themes/zzo/exampleSite/content . cp -rf themes/zzo/exampleSite/static .   修改基本信息和默认语言：config/_default/config.toml 1 2 3 4  baseURL = \u0026#34;https://carbonshow.github.io\u0026#34; title = \u0026#34;小虫飞飞\u0026#34; theme = \u0026#34;zzo\u0026#34; defaultContentLanguage = \u0026#34;zh\u0026#34;   多语言配置，仅保留中英文：config/_default/languages.toml 1 2 3 4 5 6 7 8 9 10 11 12 13  [zh] title = \u0026#34;小虫飞飞大本营\u0026#34; languageName = \u0026#34;中文\u0026#34; languageDir = \u0026#34;ltr\u0026#34; contentDir = \u0026#34;content/zh\u0026#34; weight = 1 [en] title = \u0026#34;小虫飞飞大本营\u0026#34; languageName = \u0026#34;English\u0026#34; languageDir = \u0026#34;ltr\u0026#34; contentDir = \u0026#34;content/en\u0026#34; weight = 2   菜单栏配置，每种语言一个，以中文为例： menus.zh.toml 1 2 3 4 5 6 7 8 9 10 11  [[main]] identifier = \u0026#34;about\u0026#34; name = \u0026#34;关于\u0026#34; url = \u0026#34;about\u0026#34; weight = 1 [[main]] identifier = \u0026#34;posts\u0026#34; name = \u0026#34;探险笔记\u0026#34; url = \u0026#34;posts\u0026#34; weight = 2   内容目录配置 content 目录下保存博客的实际内容，每种语言一个子目录。可以将 ko 重命名为 cn 。\n注意每个目录中的 _index.md 是对该模块的整体配置，可以参考注释结合自己需要修改。\n运行本地查看：http://localhost:1313 1  hugo server -D       'use strict'; var containerId = JSON.parse(\"\\\"6de83d5c16c2c00b\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; } \n    静态页面托管——Github  利用 Github Pages 托管并展示生成的静态页面，做到了提交后立即生效，非常适合 git 用户。 Pages 有两种类型：\n  面向用户和组织\n  面向具体工程\n  博客是一个平台，内容比较丰富可能并不针对某个具体的工程，所以选择前者。基本流程如下所示。 Step 1: Github 创建自己的账号 貌似是废话，github 官网直接注册即可， 小虫 的账号是 carbonshow ，后面以此为例展开介绍。 Step 2: Github 创建第一个 repository ，用来保存 =hugo= 工程，比如：mylog 创建 blog 仓库:   创建完毕后，clone 到本地然后按照上面的介绍建立 hugo 工程。\n1  git clone https://github.com/carbonshow/myblog.git    内容完成后大概是这个样子： blog 结构预览:   Step 3: Github 创建第二个 repository ，用来保存 静态页面 也就是对外展示的页面，仓库名建议采用： username.github.io 的形式，比如 carbonshow.github.io Step 4: 本地进入 myblog 目录，将 carbonshow.github.io 以 submoudle 的形式添加到 public 子目录中保存生成的页面。 1  git submodule add -b master https://github.com/carbonshow/carbonshow.github.io.git public    Step 5: myblog 目录内运行 hugo 会生成页面到 public 目录中。进入该目录提交并 push 到远程仓库。 Step 6: 生成和提交过程可以简化为 bash 脚本，将下述代码放入 deploy.sh 文件执行即可。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #!/bin/sh  # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Build the project. hugo # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master    Step 7: 登陆 https://carbonshow.github.io 就可以看到想要的内容啦   小结  上面介绍了基本流程，核心还是内容的创作。希望大家都能顺利搭建，然后愉快地开始码字吧。哦，顺便推荐个制作 icon 的网站：https://www.favicon-generator.org/ ，一张图片就能转化成针对不同平台的适配图，且和 zzo 主题无缝对接，大家可以试试哦。\n  ","description":"使用 Hugo 将 Markup 文档转化为静态页面，并通过 github 托管展示","id":1,"section":"posts","tags":["hugo","github","blog"],"title":"基于hugo 在github 上搭建个人博客","uri":"https://carbonshow.github.io/zh/posts/hugo_github/"},{"content":"  Pandas 是 Python 界最流行的数据统计基础库之一。但像我这样和 Java/Scala 打交道的人，还是期望 JVM 有类似的解决方案。网上一搜发现生态还是很丰富的：大数据领域的 Spark ，支持 GUI 的数据挖掘套件 weka ，主攻机器学习的 smile ，擅长聚合变换的 joinery 等等。但小虫最终还是选择了简约而不简单的 tablesaw 。缘由还得从那句老话说起： 离开场景谈应用都是耍流氓 。\n 应用场景  数据：处理流程   工作环境:   小虫在工作中使用 Spark 将业务产生的海量用户行为按模块加工：过滤冗余/简单汇总，并导出至 PostgreSQL 的不同表中，存储正交化的基础数据。比如用户的登陆/购买行为分别记录到， LoginTable 和 ShoppingTable 。然后在 二次处理 模块中建立不同服务，比如 S1，S2。S1 直接访问数据库，S2 的既要访问数据库，又要访问 S1 的统计结果，还要依赖本地的 csv 文件。\n  计算需求分类    查询计算。整表查询，列的统计，变换等。\n  筛选排序。条件查询，自定义多重排序等。\n  聚合连接。比如 LoginTable 记录终端类型：android/iOS； ShoppingTable 记录了用户购买物品的种类和数量。当要对比不同终端用户购买行为差异时，就要将两个表连接并按终端类型聚合。\n  模型验证。评估决策需要的分析模型多变，要经过反复调整得到最终结果。关键在于 快速迭代 。\n    特点汇总     纬度 特点 说明     数据量 较小单机可承载 原始数据由 spark 汇总   格式 数据库，本地 csv，json json 多来自于 restful 服务   计算 增删改查，条件查询，表连接，聚合，统计 内置算子越多，扩展性越高越好   交互 输出到指定格式，可视化，可交互 等 网页渲染，终端，Jupyter Notebook   集成 轻量，以库而非服务的形式 便于嵌入进程和其他逻辑交互     数据处理场景特点       为何选择 tablesaw   很简单，就因为它完美契合小虫的应用场景。借用 tablesaw官网 的特性列表：\n  数据导入：RDBMS，Excel，CSV，Json，HTML 或固定宽度文件。除了支持本地访问，还支持通过 http，S3 等远程访问。\n  数据导出：CSV，Json，HTML 或固定宽度文件。\n  表格操作：类似 Pandas DataFrame ，增删改查，连接，排序。\n  以上是基础功能，小虫觉得下面几个点更有意思：\n  基于Plotly 的可视化框架。摆脱 java 的 UI 系统，更好的和 Web 对接。支持 2D、3D 视图，图表类型也很丰富：曲线，散点，箱形统计，蜡烛图，热力图，饼状图等。更重要的是：\n  交互式图表。特别适合多种数据集对比，以及三维视角旋转。\n  图表导出为字符串形式 Javascript 。方便结合 Web Service 渲染 html。\n    与 smile 对接。tablesaw 可将表格导出为 smile 识别的数据格式，便于利用其强大的机器学习库。\n  好了，说了这么多，直接上干货吧。\n  基本应用  安装  tablesaw 包含多个库，小虫推荐安装 tablesaw-core 和 tablesaw-jsplot 。前者是基础库，后者用于渲染图表。其它如 tablesaw-html, tablesaw-json, tablesaw-breakerx 主要是对数据格式变化的支持，可按需选择。其实结合需求写两行代码就行，轻量又灵活。以 tablesaw-core 为例说明 Jar 包安装方法：\n maven sbt  1 2 3 4 5 6  \u0026lt;!-- https://mvnrepository.com/artifact/tech.tablesaw/tablesaw-core --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;tech.tablesaw\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tablesaw-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.37.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;     1 2  // https://mvnrepository.com/artifact/tech.tablesaw/tablesaw-core libraryDependencies += \u0026#34;tech.tablesaw\u0026#34; % \u0026#34;tablesaw-core\u0026#34; % \u0026#34;0.37.3\u0026#34;       'use strict'; var containerId = JSON.parse(\"\\\"dfb000e0c7c6500a\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; } \n  表格创建   两种基本方式：\n  从数据源读取直接创建\n  创建空表格编码增加列或行\n  表格创建流程:    下面先定义需要处理的 csv 文件格式。第一列为日期，第二列为姓名，第三列为工时（当日工作时长，单位是小时），第四列为报酬（单位是元）。然后举三个典型例子来说明导入的不同方式。\n1. CSV 直接导入  1 2 3 4 5 6 7 8  // 读取csv文件input.csv 自动推测schema val tbl = Table.read().csv(\u0026#34;input.csv\u0026#34;) // 产看读入的表格内容 println(tbl.printAll()) // 查看schema println(tbl.columnArray().mkString(\u0026#34;\\n\u0026#34;))       date name 工时 报酬     2019-01-08 tom 8 1000   2019-01-09 jerry 7 500   2019-01-10 张三 8 999   2019-01-10 jerry 8 550   2019-01-10 tom 8 1000   2019-01-11 张三 6 800   2019-01-11 李四 12 1500   2019-01-11 王五 8 900   2019-01-11 tom 6.5 800     输出表格内容   可以发现能够比较完美的推测，并对中文支持良好。输出 schema 为： Date column: date\nString column: name\nDouble column: 工时\nInteger column: 报酬 tablesaw 目前支持的数据类型有以下几种：SHORT, INTEGER, LONG ,FLOAT ,BOOLEAN ,STRING ,DOUBLE ,LOCAL_DATE ,LOCAL_TIME ,LOCAL_DATE_TIME ,INSTANT, TEXT, SKIP。绝大部分列和普通数据表类型没有差异，为一需要强调的是：\n  INSTANT。可以精确到纳秒的时间戳，自 Java 8 引入。\n  SKIP。指定列忽略不读入。\n    2. 指定 schema 从 CSV 导入  有时自动推测并不会非常精准，比如期望使用 LONG ，但识别为 INTEGER ；或在读入后追加数据时类型会有变化，比如报酬读入是整型但随后动态增加会有浮点数据。这时就需要预先设定 csv 的 schema ，这时可以利用 tablesaw 提供的 CsvReadOptions 实现。比如预先设置报酬为浮点：\n1 2 3 4 5 6 7 8 9 10  import tech.tablesaw.api.ColumnType import tech.tablesaw.io.csv.CsvReadOptions // 按序指定csv 各列的数据类型 val colTypes: Array[ColumnType] = Array(ColumnType.LOCAL_DATE, ColumnType.STRING, ColumnType.DOUBLE, ColumnType.DOUBLE) val csvReadOptions = CsvReadOptions.builder(\u0026#34;demo.csv\u0026#34;).columnTypes(colTypes) val tbl = Table.read().usingOptions(csvReadOptions) // 查看schema println(tbl.columnArray().mkString(\u0026#34;\\n\u0026#34;))    输出 schema 为： Date column: date\nString column: name\nDouble column: 工时\nDouble column: 报酬\n  3. 编码设定 schema 和数据填充  该方法适合各种场景，可以运行时从不同数据源导入数据。 基本流程是：\n 创建空表格，同时设定名称 设定 schema：向表格中按序增加指定了 =名称= 和 =数据类型= 的列。 向表格中按行追加数据。每行中的元素分别添加到指定列中。   将之前的例子做些变化，假设数据来自于网络，序列化到本地内存的数据结构为：\n1 2  // 以case class 的形式定义数据源转化到本地的内存结构 case class RowData(date: LocalDate, name: String, workTime: Double, salary: Double)    创建一个函数将获取的数据集合添加到表格中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // @param tableName 表格名称 // @param colNames 表格各列的名称列表 // @param colTypes 表格各列的数据类型列表 // @param rows 列数据 def createTable(tblName: String, colNames: Seq[String], colTypes: Seq[ColumnType], rows: Seq[RowData]): Table = { // 创建表格设定名称  val tbl = Table.create(tblName) // 创建schema ：按序增加列  val colCnt = math.min(colTypes.length, colNames.length) val cols = (0 until colCnt).map { i =\u0026gt; colTypes(i).create(colNames(i)) } tbl.addColumns(cols: _*) // 添加数据  rows.foreach { row =\u0026gt; tbl.dateColumn(0).append(row.date) tbl.stringColumn(1).append(row.name) tbl.doubleColumn(2).append(row.workTime) tbl.doubleColumn(3).append(row.salary) } tbl }    上面的说明了数据添加的完整过程：创建表格，增加列，列中追加元素。基于这三个基本操作基本可以实现所有的创建和形变。\n    列处理  列操作是表格处理的基础。前面介绍了列的数据类型，名称设置和元素追加，下面继续介绍几个基础操作。\n1. 遍历与形变  比如按序输出 demo 表格中所有记录的姓名：\n1 2 3 4 5 6 7 8 9 10  // 获取姓名列，根据列名索引 val nameCol = tbl.stringColumn(\u0026#34;name\u0026#34;) // 根据行号遍历 (0 until nameCol.size()).foreach( i =\u0026gt; println(nameCol.get(i)) ) // 直接使用column 提供的遍历接口 nameCol.forEacch(println)    除了遍历外，另一种常见应用是将列形变到另外一列：类型不变值变化；类型变化。以工时为例，我们将工时不小于 8 则视为全勤：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  // 根据列的索引获取工时一列 val workTimeCol = tbl.doubleColumn(2) // 形变1: map，输出列类型与输入列保持一致 val fullTimeCol = workTimeCol.map { time =\u0026gt; // 工时类型是Double，因此需要将形变结果也转化为 Double，否则编译失败  if (time \u0026gt;= 8) 1.0 else 0.0 } // 形变 2: mapInto，输入/输出列的数据类型可以不同，但需提前创建大小相同的目标列 val fullTimeCol = BooleanColumn.create(\u0026#34;全勤\u0026#34;, workTimeCol.size()) // 创建记录全勤标签的Boolean列 val mapFunc: Double2BooleanFunction = (workTime: Double) =\u0026gt; workTime \u0026gt;= 8.0 // 基于SAM 创建映射函数 workTimeCol.mapInto(mapFunc, fullTimeCol) // 形变 tbl.addColumns(fullTimeCol) // 将列添加到表格中       date name 工时 报酬 全勤     2019-01-08 tom 8 1000 true   2019-01-09 jerry 7 500 false   2019-01-10 张三 8 999 true   2019-01-10 jerry 8 550 true   2019-01-10 tom 8 1000 true   2019-01-11 张三 6 800 false   2019-01-11 李四 12 1500 true   2019-01-11 王五 8 900 true   2019-01-11 tom 6.5 800 false     输出结果     2. 列运算  tablesaw 提供了丰富的针对列的运算函数，而且针对不同数据类型提供了不同特化接口。建议优先查阅 API 文档，最后考虑写代码。这里介绍几个大类：\n  多列交叉运算。比如一列中所有元素和同一数据计算，或者两列元素按序交叉计算。比如每人的时薪：\n  1 2  // 第三列报酬除以第二列工时得到时薪 tbl.doubleColumn(3).divide(tbl.doubleColumn(2))      单列的统计。均值，标准差，最大 N 个值，最小 N 个值，窗口函数等。\n  1 2  // 第三列报酬的标准差 tbl.doubleColumn(3).workTimeCol.standardDeviation()      排序。数值，时间，字符串类型默认支持增序、降序，也支持自定义排序。\n    3. 过滤  tablesaw 对列的过滤条件定义为 Selection ，不同的条件可以按“与、或、非”组合。每种类型的列均提供 \u0026#34;is\u0026#34; 作为前缀的接口直接生成条件。下面举个例子，找到工作时间在 2019-01-09 - 2019-01-10 之间工时等于 8 且报酬小于 1000 的所有记录：\n1 2 3 4 5 6 7 8 9 10 11  // 设置时间的过滤条件 val datePattern = DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd\u0026#34;) val dateSel = tbl.dateColumn(0) .isBetweenIncluding(LocalDate.parse(\u0026#34;2019-01-09\u0026#34;, datePattern), LocalDate.parse(\u0026#34;2019-01-10\u0026#34;, datePattern)) // 设置工时过滤条件 val workTimeSel = tbl.doubleColumn(2).isEqualTo(8.0) // 设置报酬过滤条件 val salarySel = tbl.doubleColumn(3).isLessThan(1000) // 综合各条件过滤表格 tbl.where(dateSel.and(workTimeSel).and(salarySel))       date name 工时 报酬 全勤     2019-01-10 张三 8 999 true   2019-01-10 jerry 8 550 true     输出结果符合预期         表格处理  除了基础操作可以参考官网说明外，有三种表格的操作特别值得一提：连接，分组聚合，分表。\n连接  将有公共列名的两个表连接起来，基本方式是以公共列为 key，将各表同行其它列数据拼接起来生成新表。根据方式的不同组合有所差异：\n  inner. 公共列中的数据取交集，其他过滤。\n  outer. 公共列中的数据取并集，缺失的数据设置默认空值。具体又可以分为三类：\n  leftOuter. 结果表公共列数据与左侧表完全相同，不在其中的过滤，缺失的设置空值。\n  rightOuter. 结果表公共列数据与右侧表完全相同，不在其中的过滤，缺失的设置空值。\n  fullOuter. 结果表公共列数据为两个表的并集，缺失的设置空值。\n    举个例子，增加一个新表 tbl2 记录每人的工作地点：\n   name 地点     张三 总部   李四 门店 1   王五 门店 2     tbl2: 工作地点   采用 inner 方式和 demo 表连接：\n1  val tbl3 = tbl.joinOn(\u0026#34;name\u0026#34;).inner(tbl2)       date name 工时 报酬 全勤 地点     2019-01-10 张三 8 999 true 总部   2019-01-11 张三 6 800 false 总部   2019-01-11 李四 12 1500 true 门店 1   2019-01-11 王五 8 900 true 门店 2     tbl3   可以发现，按照 name 的交集连接，tom 和 jerry 都被过滤掉了。\n  分组聚合  类似于 SQL 中的 groupby，接口为： tbl.summarize(col1, col2, col3, aggFunc1, aggFunc2 ...).by(groupCol1, groupCol2) 。其中 by 的参数表示分组列名集合。summarize 的 col1, col2, col3 表示分组后需要被聚合处理的列名集合， aggFunc1, aggFunc2 表示聚合函数，会被用于所有的聚合列。举个例子计算每人的总报酬：\n1  tbl3.summarize(\u0026#34;报酬\u0026#34;, sum).by(\u0026#34;name\u0026#34;)       name Sum [报酬]     tom 2800   jerry 1050   张三 1799   李四 1500   王五 900     报酬汇总     分表  和分组聚合不同，按列分组后，可能并不需要将同组数据聚合为一个值，而是要保存下来做更加复杂的操作，这时就需要分表。接口很简单： tbl.splitOn(col ...) 设定分表的列名集合。比如：\n1 2  // 按照名称和地点分表，并将生成的各个子表保存到 List 中 tbl.splitOn(\u0026#34;name\u0026#34;, \u0026#34;地点\u0026#34;).asTableList()        可视化  tablesaw 可以将表格导出为交互式 html，也支持调试时直接调研调用浏览器打开，并针对不同类型图表做了个性化封装。举个简单例子，查看每人报酬的时间变化曲线：\n1 2 3 4  //含义是：将tbl 按照 name 列分组，以 date 列为时间轴，显示 报酬 的变化曲线 //并将图表的名称设置为：薪酬变化曲线 val fig = TimeSeriesPlot.create(\u0026#34;薪酬变化曲线\u0026#34;, tbl, \u0026#34;date\u0026#34;, \u0026#34;报酬\u0026#34;, \u0026#34;name\u0026#34;) Plot.show(fig)     薪酬变化曲线:   其它类型的图表还有很多，使用方法大同小异，只需根据官方文档传入正确参数即可。\n  小结  小虫向大家简单介绍了 tablesaw 的功能和使用方法，从我自己的使用经验而言，我最喜欢它的的地方在于：\n  api 接口的统一，清晰\n  交互式图表生成简单，能够和 web 对接\n  此外， tablesaw 的开发和维护也如火如荼，期待后续有更多的有趣的功能添加进来。\n  ","description":"jvm中数据表分析/展示利器","id":3,"section":"posts","tags":["pandas","jvm","scala","dataframe"],"title":"JVM上的Pandas：tablesaw","uri":"https://carbonshow.github.io/zh/posts/tablesaw/"},{"content":" 主要内容  小虫飞飞大本营 以 技术 为主要话题，将笔者在日常工作、无聊闲逛中遇到的有趣内容整理出来。一方面帮助自己归纳总结，一方面期待给更多朋友带来有益的思考。内容主要包含以下几类：\n  编程语言。当前主流语言如： jvm系 , C/C++ , C# , Python , Golang 等；也包含一些小众但在具体领域有专长的语言，比如： Lisp , R 等。\n  分布式系统。包含架构设计，组件应用，网络底层通信，服务治理，CAP 问题解决等。\n  大数据处理。海量数据离线、实时计算的解决方案，具体结合 spark/storm/flink 介绍底层原理和实际应用。\n  机器学习和推荐系统。网络中关于基础算法和内容有丰富资源， 小虫 会侧重于实际问题的思路和解决方案。\n  工具。大家看博客，撸代码都是为了增长学识和能力。除了专业技能自身，学习的方法、使用的工具也很重要。 小虫 会将业界反馈良好的利器分享出来，结合具体场景描述利弊。\n    关于小虫  也许有人好奇，技术死宅为什么会起一个 小虫飞飞 的称号，故意卖萌吗？对啊，彪形大汉内心不能是一个小白兔吗？其实。。。。。。。。。。真的不是这样的。 小虫 主要还是来自年幼时两段深刻的记忆：\n  《水浒传》。特别是武松打虎这一段，其中关于老虎描述是：云生从龙，风生从虎。龙嘛比较虚幻，虎则是切实存在的兽中之王。因此特别想 \u0026#34;狐假虎威\u0026#34; ，不过还是要含蓄点，所以取名众梁山好汉口中的： 大虫 。\n  一部动画片的主题曲：有一只小蜜峰，飞到西呀飞到东，嗡嗡嗡嗡，嗡嗡嗡嗡，不怕雨也不怕风。很朴实简单，但却塑造了一个勇敢、不畏艰险的英雄形象。这时我才知道，身形的大小不重要，关键在：态度和坚持。\n   技术本来就艰深复杂，但同样充满乐趣。大家几乎都有过被bug 折腾得死去活来的惨痛经历，那种百思不得其解的体验就像是丛林中迷途的蚂蚁。但总有柳暗花明的时候，也许一个转瞬即逝的小想法就像萤火虫给黑暗的旅途带来光明，令人豁然开朗。这种舒爽相必也是大家负重前行的动力之一吧。\n 总之，希望 小虫 能伴您一起在技术丛林中探险，也许 99% 的过程都很枯燥乏味，但只要有那1% 的收获也就不负此行了吧。\n  ","description":"关于小虫飞飞大本营内容和背景的简单介绍","id":4,"section":"","tags":null,"title":"关于","uri":"https://carbonshow.github.io/zh/about/"}]